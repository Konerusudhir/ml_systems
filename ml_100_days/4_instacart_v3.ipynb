{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instacart Product recommendations\n",
    "Kaggle Competition: https://www.kaggle.com/c/instacart-market-basket-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv(\"datasets/instacart-market-basket-analysis/products.csv\")\n",
    "print(products.head(2))\n",
    "print(f\"{products.shape}\")\n",
    "\n",
    "departments = pd.read_csv(\"datasets/instacart-market-basket-analysis/departments.csv\")\n",
    "print(departments.head(2))\n",
    "print(f\"{departments.shape}\")\n",
    "\n",
    "aisles = pd.read_csv(\"datasets/instacart-market-basket-analysis/aisles.csv\")\n",
    "print(aisles.head(2))\n",
    "print(f\"{aisles.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv(\"datasets/instacart-market-basket-analysis/orders.csv\")\n",
    "print(orders.head(2))\n",
    "print(f\"{orders.shape}\")\n",
    "\n",
    "orders_products_prior = pd.read_csv(\"datasets/instacart-market-basket-analysis/order_products__prior.csv\")\n",
    "print(orders_products_prior.head(2))\n",
    "print(f\"{orders_products_prior.shape}\")\n",
    "\n",
    "orders_products_train = pd.read_csv(\"datasets/instacart-market-basket-analysis/order_products__train.csv\")\n",
    "print(orders_products_train.head(2))\n",
    "print(f\"{orders_products_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal is to predict which previously ordered items will be in next user order\n",
    "# Extract a small sample set and perform EDA\n",
    "# Featues\n",
    "# Model Design:\n",
    "# Input : [User , Product] -> [Probability of ordering again]\n",
    "# \n",
    "# Features:\n",
    "# User: Counts: Total Orders, Order frequency, Avg unique products, Avg total items,       \n",
    "# Product: Avg order items, Order frequency\n",
    "# User-product: Order frequency per order, Order frequency per days\n",
    "# Label: Reordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exrtact prior orders for processing features\n",
    "prior_orders = orders[orders.eval_set=='prior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_total_orders = prior_orders.groupby('user_id')['order_id'].count().reset_index(name='u_total_orders')\n",
    "print(u_total_orders.head(2))\n",
    "u_history = prior_orders.groupby('user_id')['days_since_prior_order'].sum().reset_index(name='u_history_days')\n",
    "print(u_history.head(2))\n",
    "\n",
    "u_total_orders = u_total_orders.merge(u_history, on='user_id', how='left')\n",
    "u_total_orders['u_order_frequency_days'] = u_total_orders['u_history_days']/u_total_orders['u_total_orders']\n",
    "u_features = u_total_orders\n",
    "\n",
    "print(u_features.head(2))\n",
    "print(u_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_order_products = pd.merge(prior_orders, orders_products_prior, on=\"order_id\", how=\"left\")\n",
    "print(prior_order_products.head(2))\n",
    "print(f\"{prior_order_products.shape}\")\n",
    "\n",
    "p_total_orders = prior_order_products.groupby('product_id')['order_id'].count().reset_index(name='p_total_orders')\n",
    "print(p_total_orders.head(2))\n",
    "\n",
    "p_order_frequency_per_order = prior_order_products.groupby('product_id')['reordered'].mean().reset_index(name='p_reorder_rate_per_order')\n",
    "print(p_order_frequency_per_order.head(2))\n",
    "\n",
    "p_features = p_total_orders.merge(p_order_frequency_per_order, on='product_id', how='left')\n",
    "print(p_features.head(2))\n",
    "print(p_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User x Product Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uxp_reorder_rate = prior_order_products.groupby(['user_id', 'product_id'])['reordered'].mean().reset_index(name='uxp_reorder_rate_per_order')\n",
    "\n",
    "# uxp_reorders = prior_order_products.groupby(['user_id', 'product_id']).size().reset_index(name='uxp_total_orders')\n",
    "\n",
    "# uxp_reorder_rate = uxp_reorder_rate.merge(uxp_reorders, on=['user_id', 'product_id'], how='left')\n",
    "\n",
    "print(uxp_reorder_rate.head(2))\n",
    "print(uxp_reorder_rate.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders train\n",
    "# combine with user, product and uxp features\n",
    "# remove unnecessayr columns\n",
    "# split to train and test\n",
    "\n",
    "orders_train = orders[orders.eval_set=='train']\n",
    "print(orders_train.shape)\n",
    "\n",
    "order_products_train = pd.merge(orders_train, orders_products_train, on='order_id', how='left')\n",
    "print(order_products_train.head(2))\n",
    "print(order_products_train.shape)\n",
    "\n",
    "# merge user feaures\n",
    "features_dataset = order_products_train.merge(u_features, on='user_id', how='left')\n",
    "print(features_dataset.head(2))\n",
    "\n",
    "# merge product features\n",
    "features_dataset = features_dataset.merge(p_features, on='product_id', how='left')\n",
    "print(features_dataset.head(2))\n",
    "\n",
    "# merge uxp features\n",
    "features_dataset = features_dataset.merge(uxp_reorder_rate, on=['user_id', 'product_id'], how='left')\n",
    "print(features_dataset.head(2))\n",
    "\n",
    "# remove unwanted features\n",
    "features_dataset = features_dataset.drop(['eval_set', 'order_id', 'product_id', 'user_id'], axis=1)\n",
    "\n",
    "features_dataset[\"p_reorder_rate_per_day\"] = features_dataset[\"p_reorder_rate_per_order\"] * features_dataset[\"u_order_frequency_days\"]\n",
    "\n",
    "print(features_dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Nan values\n",
    "print(features_dataset.isnull().sum())\n",
    "features_dataset = features_dataset.fillna(0)\n",
    "features_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset.groupby('reordered').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPlit and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_STATE = 7\n",
    "TEST_SIZE = 0.2 \n",
    "\n",
    "train, test = train_test_split(features_dataset, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "X_train = train.drop('reordered', axis=1)\n",
    "Y_train = train['reordered']\n",
    "\n",
    "X_test = test.drop('reordered', axis=1)\n",
    "Y_test = test['reordered']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(Y_test, Y_pred):\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix(Y_test, Y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(Y_test, Y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(Y_test, Y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(Y_test, Y_pred)}\")\n",
    "    print(f\"F1 Score: {f1_score(Y_test, Y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=200, random_state=RANDOM_STATE)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "metrics(Y_test, Y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_depth=10, \n",
    "    min_samples_leaf=10,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "metrics(Y_test, Y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=10,\n",
    "    max_depth=10, \n",
    "    min_samples_leaf=10,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "metrics(Y_test, Y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc_model = GradientBoostingClassifier(\n",
    "    loss='log_loss',\n",
    "    n_estimators=10,\n",
    "    max_depth=10, \n",
    "    min_samples_leaf=10,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "gbc_model.fit(X_train, Y_train)\n",
    "Y_pred = gbc_model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "metrics(Y_test, Y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionTreeClassifier = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "decisionTreeClassifier.fit(X_train, Y_train)\n",
    "Y_pred = decisionTreeClassifier.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "metrics(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "# plot a large image of the tree\n",
    "\n",
    "plt.figure(figsize=(20, 10))  \n",
    "tree.plot_tree(\n",
    "    decisionTreeClassifier, \n",
    "    max_depth=3, \n",
    "    feature_names = X_train.columns, \n",
    "    class_names = ['0', '1'],\n",
    "    filled=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importances\n",
    "importances = decisionTreeClassifier.feature_importances_\n",
    "\n",
    "# sort the importances\n",
    "sorted_index = importances.argsort()\n",
    "\n",
    "# create labels\n",
    "labels = X_train.columns[sorted_index]\n",
    "\n",
    "# create plot\n",
    "for i in range(len(sorted_index) - 1, -1, -1):\n",
    "    print(f\"{labels[i]} \\t {importances[sorted_index[i]].round(4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode dow with one hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "dow_encoded = encoder.fit_transform(features_dataset[['order_dow']]).toarray()\n",
    "dow_encoded = pd.DataFrame(dow_encoded, columns=[f\"dow_{i}\" for i in range(dow_encoded.shape[1])])\n",
    "\n",
    "features_dataset = pd.concat([features_dataset, dow_encoded], axis=1)\n",
    "\n",
    "# remove dow\n",
    "features_dataset = features_dataset.drop('order_dow', axis=1)\n",
    "\n",
    "\n",
    "def get_train_test_split(features_dataset):\n",
    "    train, test = train_test_split(features_dataset, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "    X_train = train.drop('reordered', axis=1)\n",
    "    Y_train = train['reordered']\n",
    "\n",
    "    X_test = test.drop('reordered', axis=1)\n",
    "    Y_test = test['reordered']\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = get_train_test_split(features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=200, random_state=RANDOM_STATE)\n",
    "lr.fit(X_train, Y_train)\n",
    "Y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "metrics(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode hour of day to early morning, morning, afternoon, evening, night\n",
    "\n",
    "def encode_hour_of_day(hour):\n",
    "    if hour < 6:\n",
    "        return 'early_morning'\n",
    "    elif hour < 12:\n",
    "        return 'morning'\n",
    "    elif hour < 18:\n",
    "        return 'afternoon'\n",
    "    elif hour < 21:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "features_dataset['order_hour_of_day'] = features_dataset['order_hour_of_day'].apply(encode_hour_of_day)\n",
    "\n",
    "# encode hour of day with one hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "hour_encoded = encoder.fit_transform(features_dataset[['order_hour_of_day']]).toarray()\n",
    "hour_encoded = pd.DataFrame(hour_encoded, columns=[f\"hour_{i}\" for i in range(hour_encoded.shape[1])])\n",
    "\n",
    "features_dataset = pd.concat([features_dataset, hour_encoded], axis=1)\n",
    "\n",
    "# remove hour of day\n",
    "features_dataset = features_dataset.drop('order_hour_of_day', axis=1)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = get_train_test_split(features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=200, random_state=RANDOM_STATE)\n",
    "lr.fit(X_train, Y_train)\n",
    "Y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "metrics(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTreeClassifier = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "decisionTreeClassifier.fit(X_train, Y_train)\n",
    "Y_pred = decisionTreeClassifier.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "metrics(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importances\n",
    "importances = decisionTreeClassifier.feature_importances_\n",
    "\n",
    "# sort the importances\n",
    "sorted_index = importances.argsort()\n",
    "\n",
    "# create labels\n",
    "labels = X_train.columns[sorted_index]\n",
    "\n",
    "# create plot\n",
    "for i in range(len(sorted_index) - 1, -1, -1):\n",
    "    print(f\"{labels[i]} \\t {importances[sorted_index[i]].round(4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
