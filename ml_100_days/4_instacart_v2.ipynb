{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv(\"datasets/instacart-market-basket-analysis/products.csv\")\n",
    "print(products.head(2))\n",
    "print(f\"{products.shape}\")\n",
    "\n",
    "departments = pd.read_csv(\"datasets/instacart-market-basket-analysis/departments.csv\")\n",
    "print(departments.head(2))\n",
    "print(f\"{departments.shape}\")\n",
    "\n",
    "aisles = pd.read_csv(\"datasets/instacart-market-basket-analysis/aisles.csv\")\n",
    "print(aisles.head(2))\n",
    "print(f\"{aisles.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv(\"datasets/instacart-market-basket-analysis/orders.csv\")\n",
    "print(orders.head(2))\n",
    "print(f\"{orders.shape}\")\n",
    "\n",
    "orders_products_prior = pd.read_csv(\"datasets/instacart-market-basket-analysis/order_products__prior.csv\")\n",
    "print(orders_products_prior.head(2))\n",
    "print(f\"{orders_products_prior.shape}\")\n",
    "\n",
    "orders_products_train = pd.read_csv(\"datasets/instacart-market-basket-analysis/order_products__train.csv\")\n",
    "print(orders_products_train.head(2))\n",
    "print(f\"{orders_products_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal is to predict which previously ordered items will be in next user order\n",
    "# Extract a small sample set and perform EDA\n",
    "# Featues\n",
    "# Model Design:\n",
    "# Input : [User , Product] -> [Probability of ordering again]\n",
    "# \n",
    "# Features:\n",
    "# User: Counts: Total Orders, Order frequency, Avg unique products, Avg total items,       \n",
    "# Product: Avg order items, Order frequency\n",
    "# Label: Reordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exrtact prior orders for processing features\n",
    "prior_orders = orders[orders.eval_set=='prior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_total_orders = prior_orders.groupby('user_id')['order_id'].count().reset_index(name='u_total_orders')\n",
    "print(u_total_orders.head(2))\n",
    "u_history = prior_orders.groupby('user_id')['days_since_prior_order'].sum().reset_index(name='u_history_days')\n",
    "print(u_history.head(2))\n",
    "\n",
    "u_total_orders = u_total_orders.merge(u_history, on='user_id', how='left')\n",
    "u_total_orders['u_order_frequency_days'] = u_total_orders['u_history_days']/u_total_orders['u_total_orders']\n",
    "u_features = u_total_orders\n",
    "\n",
    "print(u_features.head(2))\n",
    "print(u_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_order_products = pd.merge(prior_orders, orders_products_prior, on=\"order_id\", how=\"left\")\n",
    "print(prior_order_products.head(2))\n",
    "print(f\"{prior_order_products.shape}\")\n",
    "\n",
    "p_total_orders = prior_order_products.groupby('product_id')['order_id'].count().reset_index(name='p_total_orders')\n",
    "print(p_total_orders.head(2))\n",
    "\n",
    "p_order_frequency_per_order = prior_order_products.groupby('product_id')['reordered'].mean().reset_index(name='p_reorder_rate_per_order')\n",
    "print(p_order_frequency_per_order.head(2))\n",
    "\n",
    "p_features = p_total_orders.merge(p_order_frequency_per_order, on='product_id', how='left')\n",
    "print(p_features.head(2))\n",
    "print(p_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User x Product Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uxp_reorder_rate = prior_order_products.groupby(['user_id', 'product_id'])['reordered'].mean().reset_index(name='uxp_reorder_rate_per_order')\n",
    "\n",
    "uxp_reorders = prior_order_products.groupby(['user_id', 'product_id']).size().reset_index(name='uxp_total_orders')\n",
    "\n",
    "print(uxp_reorder_rate.head(2))\n",
    "print(uxp_reorder_rate.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders train\n",
    "# combine with user, product and uxp features\n",
    "# remove unnecessayr columns\n",
    "# split to train and test\n",
    "\n",
    "orders_train = orders[orders.eval_set=='train']\n",
    "print(orders_train.shape)\n",
    "\n",
    "order_products_train = pd.merge(orders_train, orders_products_train, on='order_id', how='left')\n",
    "print(order_products_train.head(2))\n",
    "print(order_products_train.shape)\n",
    "\n",
    "# merge user feaures\n",
    "features_dataset = order_products_train.merge(u_features, on='user_id', how='left')\n",
    "print(features_dataset.head(2))\n",
    "\n",
    "# merge product features\n",
    "features_dataset = features_dataset.merge(p_features, on='product_id', how='left')\n",
    "print(features_dataset.head(2))\n",
    "\n",
    "# merge uxp features\n",
    "features_dataset = features_dataset.merge(uxp_reorder_rate, on=['user_id', 'product_id'], how='left')\n",
    "print(features_dataset.head(2))\n",
    "\n",
    "# remove unwanted features\n",
    "features_dataset = features_dataset.drop(['eval_set', 'order_id', 'product_id', 'user_id'], axis=1)\n",
    "\n",
    "features_dataset[\"p_reorder_rate_per_day\"] = features_dataset[\"p_reorder_rate_per_order\"] * features_dataset[\"u_order_frequency_days\"]\n",
    "\n",
    "print(features_dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Nan values\n",
    "print(features_dataset.isnull().sum())\n",
    "features_dataset = features_dataset.fillna(0)\n",
    "features_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset.groupby('reordered').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPlit and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_STATE = 7\n",
    "TEST_SIZE = 0.2 \n",
    "\n",
    "train, test = train_test_split(features_dataset, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "X_train = train.drop('reordered', axis=1)\n",
    "Y_train = train['reordered']\n",
    "\n",
    "X_test = test.drop('reordered', axis=1)\n",
    "Y_test = test['reordered']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(iterations=100):\n",
    "    model = LogisticRegression(max_iter=iterations, random_state=RANDOM_STATE)\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "train(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def train_tree():\n",
    "    model = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=10, min_samples_leaf=10)\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "train_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def train_forest():\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=10,\n",
    "        max_depth=10, \n",
    "        min_samples_leaf=10,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "train_forest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer Perceptron - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train neural network model using sklearn \n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# def train_neural_network():\n",
    "#     model = MLPClassifier(\n",
    "#         hidden_layer_sizes=(10, 5),\n",
    "#         max_iter=100,\n",
    "#         random_state=RANDOM_STATE,\n",
    "#         early_stopping=True,\n",
    "#     )\n",
    "#     model.fit(X_train, Y_train)\n",
    "#     Y_pred = model.predict(X_test)\n",
    "#     accuracy = accuracy_score(Y_test, Y_pred)\n",
    "#     print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# train_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gcc_model = GradientBoostingClassifier(\n",
    "    criterion='squared_error',\n",
    "    n_estimators=2,\n",
    "    max_depth=10, \n",
    "    min_samples_leaf=10,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "gcc_model.fit(X_train, Y_train)\n",
    "Y_pred = gcc_model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionTreeClassifier = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "decisionTreeClassifier.fit(X_train, Y_train)\n",
    "Y_pred = decisionTreeClassifier.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "# plot a large image of the tree\n",
    "\n",
    "plt.figure(figsize=(20, 10))  \n",
    "tree.plot_tree(\n",
    "    decisionTreeClassifier, \n",
    "    max_depth=3, \n",
    "    feature_names = X_train.columns, \n",
    "    class_names = ['0', '1'],\n",
    "    filled=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print SHAP values\n",
    "Shapely Additive explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import shap\n",
    "\n",
    "ex = shap.TreeExplainer(decisionTreeClassifier)\n",
    "shap_values = ex(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_x_values = shap_values.values\n",
    "print(shap_x_values.shape)\n",
    "# Reshape the shap values to be used for bar plot\n",
    "shap_x_values = shap_x_values.reshape(shap_values.shape[0], shap_values.shape[1])\n",
    "\n",
    "print(shap_x_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mean_shap_values = np.mean(np.array(shap_values), axis=0)\n",
    "# mean_shap_values = np.mean(shap_values[0], axis=0)\n",
    "# mean_shap_values = sum(shap_values) / len(shap_values)\n",
    "shap.plots.bar(mean_shap_values, feature_names=X_test.columns)\n",
    "\n",
    "# shap.plots.bar(shap_values[0])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Level Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_test is your test data\n",
    "leaf_indices = []\n",
    "leaf_values = []\n",
    "single_example = X_test[0:1]\n",
    "\n",
    "    # Get the leaf node indices for each sample\n",
    "indices = decisionTreeClassifier.apply(single_example)\n",
    "print(indices)\n",
    "# indices = tree_in_forest[0].apply(singl_example)\n",
    "# leaf_indices.append(indices)\n",
    "\n",
    "# # Extract the corresponding leaf values\n",
    "values = decisionTreeClassifier.tree_.value[indices][:,0,0]\n",
    "print(values)\n",
    "leaf_values.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print decision_path\n",
    "sparse_matrix = decisionTreeClassifier.decision_path(X_test[0:1])\n",
    "print(sparse_matrix.shape)\n",
    "print(sparse_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = decisionTreeClassifier\n",
    "n_nodes = clf.tree_.node_count\n",
    "children_left = clf.tree_.children_left\n",
    "children_right = clf.tree_.children_right\n",
    "feature = clf.tree_.feature\n",
    "threshold = clf.tree_.threshold\n",
    "values = clf.tree_.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_indicator = clf.decision_path(X_test[0:1])\n",
    "leaf_id = clf.apply(X_test[0:1])\n",
    "\n",
    "sample_id = 0\n",
    "# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\n",
    "node_index = node_indicator.indices[\n",
    "    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n",
    "]\n",
    "print(node_index)\n",
    "print(feature)\n",
    "print(threshold)\n",
    "print(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\n",
    "for node_id in node_index:\n",
    "    # continue to the next node if it is a leaf node\n",
    "    if leaf_id[sample_id] == node_id:\n",
    "        continue\n",
    "\n",
    "    # check if value of the split feature for sample 0 is below threshold\n",
    "    # print(f\"node_id: {node_id} Feature: {feature[node_id]} Threshold: {threshold[node_id]}\")\n",
    "    sample_value = X_test.iloc[sample_id, feature[node_id]]\n",
    "    # print(f\"X_test[sample_id, feature[node_id]]: {sample_value} threshold: {threshold[node_id]}\")\n",
    "\n",
    "    if X_test.iloc[sample_id, feature[node_id]] <= threshold[node_id]:\n",
    "        threshold_sign = \"<=\"\n",
    "    else:\n",
    "        threshold_sign = \">\"\n",
    "\n",
    "    print(\n",
    "        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n",
    "        \"{inequality} {threshold})\".format(\n",
    "            node=node_id,\n",
    "            sample=sample_id,\n",
    "            feature=feature[node_id],\n",
    "            value=X_test.iloc[sample_id, feature[node_id]],\n",
    "            inequality=threshold_sign,\n",
    "            threshold=threshold[node_id],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_id = clf.apply(X_test[0:1])\n",
    "print(clf.predict(X_test[0:1]))\n",
    "print(clf.predict_proba(X_test[0:1]))\n",
    "print(clf.predict_log_proba(X_test[0:1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
