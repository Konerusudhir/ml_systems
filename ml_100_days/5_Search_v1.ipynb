{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCXJW8ZdLtK1"
   },
   "source": [
    "# Faire - The Online Wholesale Marketplace & Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e88SjvpgLtK2"
   },
   "source": [
    "Welcome! Lets build a new search ranking model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQmR6dCcLtK3"
   },
   "source": [
    "### Description\n",
    "\n",
    "Knowing whether a Product in Search will be bought in advance could provide huge business value to Faire. This task is very important for purchase prediction as well as for short term user engagement prediction.\n",
    "\n",
    "- In this dataset we have sampled ~20k rows from Faire search logs. The dataset is anonymized. Before describing the dataset, lets give some preliminary knowledge of how Faire Wholesale MarketPlace Search works. Faire is a two-sided marketplace where retailers come to shop wholesale products from brands. When a retailer makes a search on the site, we call that a search request. The response is a page with many products. We assign a `request_id` to this search request response, and different pages (page number 1, 2, 3...) from the same search have different `request_id`s (i.e. `request_id` is more of a \"page id\" than a \"search session id\"). Each row in this dataset represents one single product that was impressed for that `request_id`. For each request_id you can have many results (due to this being a random sample some of them might be missed in a some cases). \n",
    "\n",
    "- Each row contains the following fields: `request_id`, `retailer_token` (anonymzed user token), `query_text` (the actual search string), `page_number`, `page_size`, `position`, `filter_string` (filters applied on top of the search), `has_product_click` (was it clicked or not). \n",
    "- We have some features from our feature store (computed using data from before the search timestamp): anything starting with `product.` is a product-level feature. We have also a few personalization features in this dataset, anything starting with `retailerbrand.` is a personalization feature and relates to a particular retailer:brand pair. \n",
    "- Note that we have personalization features only at the level of retailer(user) and the brand the product belongs to. Brands usually have many products in our marketplace, so Product <-> Brand mapping is Many:1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQmR6dCcLtK3"
   },
   "source": [
    "### Tasks\n",
    "In approximately 1 hour, please do the following:\n",
    "- Please build first a ranking model using the provided dataset and evaluate it. \n",
    "- If there is time, please implement 1 or 2 additional features, and list up to 10 more features (without implementing them).\n",
    "- If there is time, please give us an idea of the next steps, how would you improve this model if you had 1 week more, 1 month more, or 3 months more?\n",
    "\n",
    "Please write as many comments or communicate out loud your thought process. Once your time allocation is up, please send back the completed notebook in .ipynb and .pdf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1603217376889,
     "user": {
      "displayName": "Ildus Ahmadullin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaB9bqpJm6HE2WR6lDP6dLFHKsS3SpXHFabinl=s64",
      "userId": "02756298952784264409"
     },
     "user_tz": 420
    },
    "id": "c821NPAOLtK3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "error",
     "timestamp": 1603217749887,
     "user": {
      "displayName": "Ildus Ahmadullin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaB9bqpJm6HE2WR6lDP6dLFHKsS3SpXHFabinl=s64",
      "userId": "02756298952784264409"
     },
     "user_tz": 420
    },
    "id": "7eOmQIatMwca",
    "outputId": "69099e62-51d2-4a04-914e-bead825406c3"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/faire-ml-rank-small.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corr_features = data.select_dtypes(include=[np.number])\n",
    "corr = corr_features.corr()\n",
    "\n",
    "plt.figure(figsize=[10,8])\n",
    "sns.heatmap(corr)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "RANDOM_STATE = 7\n",
    "\n",
    "def prepare_data(features, scale=False):\n",
    "    # remove unwanted columns\n",
    "    numeric_features = features.select_dtypes(include=[np.number])\n",
    "    numeric_features = numeric_features.drop([\"Unnamed: 0.1\", \"Unnamed: 0\", \"retailer_token_anon\", \"rand\"], axis=1)\n",
    "    \n",
    "    numeric_features = numeric_features.fillna(0)\n",
    "\n",
    "    # print(numeric_data.describe())\n",
    "    # split the data into training and testing sets uing request_id_annon as the target variable\n",
    "    request_id_annon = numeric_features['request_id_anon'].unique()\n",
    "    X_train_rid, X_test_rid = train_test_split(request_id_annon, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "    X1 = numeric_features[numeric_features['request_id_anon'].isin(X_train_rid)]\n",
    "    X2 = numeric_features[numeric_features['request_id_anon'].isin(X_test_rid)]\n",
    "\n",
    "    X1 = X1.drop(['request_id_anon'], axis=1)\n",
    "    X2 = X2.drop(['request_id_anon'], axis=1)\n",
    "\n",
    "   \n",
    "   \n",
    "    if scale:\n",
    "        columns = X1.columns\n",
    "        scaler = StandardScaler()\n",
    "        X1 = scaler.fit_transform(X1)\n",
    "        X2 = scaler.transform(X2)\n",
    "\n",
    "        X1 = pd.DataFrame(X1, columns=columns)\n",
    "        X2 = pd.DataFrame(X2, columns=columns)\n",
    "    \n",
    "    \n",
    "    X_train = X1.drop('has_product_click', axis=1)\n",
    "    Y_train = X1['has_product_click'].astype(int)\n",
    "    X_test = X2.drop('has_product_click', axis=1)\n",
    "    Y_test = X2['has_product_click'].astype(int)\n",
    "    \n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(Y_test, Y_pred, Y_proba):\n",
    "    class_probabilities = Y_proba[:, 1]\n",
    "    print(f\"Y_pred: {Y_pred[:3]}\")\n",
    "    print(f\"Y_proba shape: {class_probabilities[:3]}\")\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix(Y_test, Y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(Y_test, Y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(Y_test, Y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(Y_test, Y_pred)}\")\n",
    "    print(f\"F1 Score: {f1_score(Y_test, Y_pred)}\")\n",
    "\n",
    "    # print pr_auc score\n",
    "    from sklearn.metrics import auc\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    precision, recall, _ = precision_recall_curve(Y_test, class_probabilities)\n",
    "    print(f\"PR AUC: {auc(recall, precision)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "def lr_model(current_data, scale=False):\n",
    "    X_train, Y_train, X_test, Y_test = prepare_data(current_data, scale)\n",
    "    lr = LogisticRegression(random_state=RANDOM_STATE)\n",
    "    lr.fit(X_train, Y_train)\n",
    "    Y_pred = lr.predict(X_test)\n",
    "    print(f\"Y_pred shape: {Y_pred.shape}\")\n",
    "    Y_proba = lr.predict_proba(X_test)\n",
    "    metrics(Y_test, Y_pred, Y_proba)\n",
    "\n",
    "    sorted_indices = lr.coef_[0].argsort()\n",
    "    for i in range(1, 11):\n",
    "        print(f\"Feature: {X_train.columns[sorted_indices[-i]]}, Importance: {lr.coef_[0][sorted_indices[-i]]}\")\n",
    "\n",
    "\n",
    "lr_model(data)\n",
    "lr_model(data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def dt_model(current_data, scale=False):\n",
    "    X_train, Y_train, X_test, Y_test = prepare_data(current_data, scale)\n",
    "    dtc = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=10, min_samples_leaf=10, criterion='entropy')\n",
    "    dtc.fit(X_train, Y_train)\n",
    "    Y_pred = dtc.predict(X_test)\n",
    "\n",
    "    metrics(Y_test, Y_pred)\n",
    "\n",
    "    sorted_indices = dtc.feature_importances_.argsort()\n",
    "\n",
    "    # print the top 10 features and their importance score\n",
    "    for i in range(1, 11):\n",
    "        print(f\"Feature: {X_train.columns[sorted_indices[-i]]}, Importance: {dtc.feature_importances_[sorted_indices[-i]]}\")\n",
    "\n",
    "\n",
    "dt_model(numeric_data)\n",
    "dt_model(numeric_data, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['query_text'] = data['query_text'].str.lower()\n",
    "data['title'] = data['title'].str.lower()\n",
    "data['description'] = data['description'].str.lower()\n",
    "# print(data[['query_text', 'title', 'description']].head(10))\n",
    "\n",
    "\n",
    "# chekc if the query_text is present in the title column\n",
    "data['query_in_title'] = data.apply(lambda x: 1 if x['query_text'] in x['title'] else 0, axis=1)\n",
    "\n",
    "data['description'].fillna(\"\", inplace=True)\n",
    "# check if the query_text is present in the description column\n",
    "data['query_text_in_description'] = data.apply(lambda x: 1 if x['query_text'] in x['description'] else 0, axis=1)\n",
    "\n",
    "# data['query_text_in_description'].value_counts()\n",
    "selected_data = data.select_dtypes(include=[np.number, bool])\n",
    "\n",
    "dt_model(selected_data)\n",
    "dt_model(selected_data, scale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Exercise-Ranking-Faire-Puya.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
